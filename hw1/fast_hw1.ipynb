{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "069c3495",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from time import time\n",
    "from functools import reduce\n",
    "\n",
    "import os\n",
    "from os.path import join\n",
    "\n",
    "def read_query(filepath):\n",
    "    tree = ET.parse(filepath)\n",
    "    questions = []\n",
    "    for p in tree.findall('topic'):\n",
    "        questions.append([x.text.strip() for x in p.findall('*')])\n",
    "    return questions\n",
    "\n",
    "def query_processing(questions):\n",
    "    questions = [x[-1][:-1].split('ã€') for x in questions]\n",
    "    res = []\n",
    "    for q in questions:\n",
    "        uni_qspace = set()\n",
    "        bi_qspace = set()\n",
    "        for words in q:\n",
    "            uni_qspace |= set(list(words))\n",
    "            bi_qspace |= set([words[i:i+2] for i in range(len(words)-1)])\n",
    "        res.append((list(uni_qspace), list(bi_qspace)))\n",
    "    return res\n",
    "\n",
    "def open_file(path):\n",
    "    tree = ET.parse(path.strip())\n",
    "    id_ = tree.find('.//id').text\n",
    "    text = ''.join([x.text.strip() for x in tree.findall('.//p')])\n",
    "    return id_, text\n",
    "    \n",
    "def BM25_score(d, terms, cand, k1=1.2, b=0.75):\n",
    "    tf = d.t2d[terms, :][:, cand].toarray()\n",
    "    return d.idf[terms].dot((tf*(k1 + 1))/(tf + k1*(1 - b + b*(d.docs_length[cand] / d.avg_length))))\n",
    "\n",
    "def get_result_list(q_id, filelist, res):\n",
    "    return [q_id[-3:], ' '.join([open_file(filelist[i])[0].lower() for i in res])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dedfb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, model_path, ntcir_path):        \n",
    "        self.filelist = []\n",
    "        \n",
    "        self.docs_length = np.zeros(0)\n",
    "        self.avg_length = 0\n",
    "        self.num_docs = 0\n",
    "        \n",
    "        self.vocabs_dict = {}\n",
    "        self.black_list = []\n",
    "        self.idf = np.zeros(0)\n",
    "        self.t2d = None\n",
    "        \n",
    "        self.start = 0\n",
    "        self.model_path = model_path\n",
    "        self.ntcir_path = ntcir_path\n",
    "        \n",
    "    def get_docs_length(self):\n",
    "        with open(join(self.model_path, 'file-list')) as f:\n",
    "            self.filelist = [join(self.ntcir_path, s.strip()) for s in f.readlines()]\n",
    "            for file_path in self.filelist:\n",
    "                tree = ET.parse(file_path)\n",
    "                text = ''.join([x.text.strip() for x in tree.findall('.//p')])\n",
    "                chinese_text = re.findall(r\"[\\u4e00-\\u9fa5']+\", text)\n",
    "                self.docs_length = np.r_[self.docs_length, np.sum([len(x) for x in chinese_text])]\n",
    "        self.avg_length = np.mean(self.docs_length)\n",
    "        self.num_docs = len(self.docs_length)\n",
    "    \n",
    "    def dump_time(self, slogan):\n",
    "        print(slogan+', total time: %06.2f sec.' % (time() - self.start))\n",
    "\n",
    "    def build(self, corpus):\n",
    "        self.start = time()\n",
    "        self.get_docs_length()\n",
    "        self.dump_time('Finish getting documents length')\n",
    "        \n",
    "        # Read inverted-file\n",
    "        all_term = pd.read_csv(join(self.model_path, 'inverted-file'), delimiter=' ', header=None, usecols=[0,1,2]).values\n",
    "        # Get the indices of lines with 3 digits\n",
    "        indices = np.where(~np.isnan(all_term[:, 2]))[0]\n",
    "        \n",
    "        self.dump_time('Finish reading inverted file')\n",
    "        \n",
    "        # Read vocab.all\n",
    "        char = pd.read_csv(join(self.model_path, 'vocab.all'), header=None, index_col=False, delimiter='\\n', quoting=3, encoding='utf-8').values.reshape(-1)\n",
    "        char_dict = dict(zip(char, np.arange(len(char), dtype=int)))\n",
    "        \n",
    "        self.dump_time('Finish reading vocabulary file')\n",
    "        \n",
    "        terms = [(char_dict[x[0]], char_dict[x[1]]) if len(x) > 1 else (char_dict[x[0]], -1) for x in corpus]\n",
    "        terms = sorted(terms, key=lambda t: (t[0], t[1]))\n",
    "        i = 0\n",
    "        row, col, data = [], [], []\n",
    "        for t1, t2 in terms:\n",
    "            w = char[t1] + char[t2] if t2 > 0 else char[t1]\n",
    "            for move, idx in enumerate(indices[i:]):\n",
    "                # Find the term\n",
    "                if all_term[idx][0] == t1 and all_term[idx][1] == t2:\n",
    "                    # Add the term to vocabs_dict\n",
    "                    self.vocabs_dict[w] = len(self.vocabs_dict)\n",
    "\n",
    "                    nqi = all_term[idx][2]\n",
    "                    self.idf = np.r_[self.idf, np.log((self.num_docs - nqi + 0.5)/(nqi + 0.5) + 1)]\n",
    "                    interval = all_term[idx+1:indices[i+move+1]].astype(int)\n",
    "\n",
    "                    row += [len(self.vocabs_dict)-1]*len(interval)\n",
    "                    col += interval[:, 0].tolist()\n",
    "                    data += interval[:, 1].tolist()\n",
    "                    break\n",
    "                    \n",
    "                # The term doesn't exist in inverted-file\n",
    "                elif all_term[idx][0] > t1:\n",
    "                    self.black_list.append(w)\n",
    "                    break\n",
    "            i += move\n",
    "            print('Processing ... %06.2f%%, total time: %06.2f sec.' % (100*(i+1)/len(indices), time() - self.start), end='\\r')\n",
    "        self.t2d = csr_matrix((data, (row, col)), shape=(len(self.idf), self.num_docs))\n",
    "        self.idf = np.array(self.idf)\n",
    "        self.dump_time('\\nFinish building dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db0df915",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self, query_file, output_file, model_path, nctir_path):\n",
    "        self.query_file = query_file\n",
    "        self.output_file = output_file\n",
    "        self.model_path = model_path\n",
    "        self.nctir_path = nctir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6eb36ba6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish getting documents length, total time: 017.06 sec.\n",
      "Finish reading inverted file, total time: 031.14 sec.\n",
      "Finish reading vocabulary file, total time: 031.16 sec.\n",
      "Processing ... 097.16%, total time: 035.60 sec.\n",
      "Finish building dataset, total time: 038.39 sec.\n",
      "Processing 20 / 20, total time: 041.00 sec.\n",
      "Finish, total time: 041.50 sec.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    start = time()\n",
    "    args = Args('/tmp2/r09922104/ir/queries/query-test.xml', './output.csv', \n",
    "                '/tmp2/r09922104/ir/model', '/tmp2/r09922104/ir/CIRB010')\n",
    "\n",
    "    questions = read_query(args.query_file)\n",
    "    queries = query_processing(questions)\n",
    "    corpus = reduce(np.union1d, [x+y for x, y in queries]).tolist()\n",
    "\n",
    "    d = Dataset(args.model_path, args.nctir_path)\n",
    "    d.build(corpus)\n",
    "\n",
    "    result = []\n",
    "    for _, query in enumerate(queries):\n",
    "        print('Processing %02d / %02d, total time: %06.2f sec.' % (_+1, len(queries), time() - start), end='\\r')\n",
    "        uni, bi = query\n",
    "        bi = np.setdiff1d(bi, d.black_list).tolist()\n",
    "        candidates = reduce(np.union1d, [d.t2d[d.vocabs_dict[x]].nonzero() for x in bi])\n",
    "        \n",
    "        query_terms = [d.vocabs_dict[x] for x in uni+bi]\n",
    "        \n",
    "        scores = BM25_score(d, query_terms, candidates)\n",
    "        rank = np.argsort(scores)\n",
    "        res = [candidates[i] for i in rank[-100:][::-1]]\n",
    "        result.append(get_result_list(questions[_][0], d.filelist, res))\n",
    "    pd.DataFrame(result).to_csv(args.output_file, header=['query_id','retrieved_docs'], index=False)\n",
    "    print('\\nFinish, total time: %06.2f sec.' % (time() - start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ir",
   "language": "python",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
